{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whs1111/bert-version/blob/master/src/CLASS/LSTM2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMb0U7ctSDcL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3029bc6-94dc-4173-b6eb-8ff35b78009f"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS8U2HuDtZix"
      },
      "source": [
        "# 新段落"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orTnaAutSYb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "373ab4db-4ff1-436e-eb5f-5ae2876dcfa5"
      },
      "source": [
        "cd gdrive/My Drive/Colab_Notebooks/code\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab_Notebooks/code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNs9soyfIdAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad860011-fb3c-4d89-eabe-82bb051e66f8"
      },
      "source": [
        "#@title\n",
        "!git clone https://github.com/NVIDIA/apex.git\n",
        "%cd apex\n",
        "!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .\n",
        "%cd ..\n",
        "\n",
        "import torch\n",
        "import apex\n",
        "from fastai.text import *\n",
        "import datetime\n",
        "run_start_time = datetime.datetime.today().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "\n",
        "LOG_PATH=Path('logs/')  \n",
        "MODEL_PATH=Path('models/') \n",
        "\n",
        "if not LOG_PATH.exists():\n",
        "  LOG_PATH.mkdir()\n",
        "import logging\n",
        "\n",
        "args = {\n",
        "    \"run_text\": \"my_test\",\n",
        "    \"max_seq_length\": 30,\n",
        "    \"do_lower_case\": True,\n",
        "    \"train_batch_size\": 16,\n",
        "    \"learning_rate\": 6e-5,\n",
        "    \"num_train_epochs\": 12.0,\n",
        "    \"warmup_proportion\": 0.002,\n",
        "    \"local_rank\": -1,\n",
        "    \"gradient_accumulation_steps\": 1,\n",
        "    \"fp16\": True,\n",
        "    \"loss_scale\": 128\n",
        "}\n",
        "\n",
        "logfile = str(LOG_PATH/'log-{}-{}.txt'.format(run_start_time, args[\"run_text\"]))\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "    datefmt='%m/%d/%Y %H:%M:%S',\n",
        "    handlers=[\n",
        "        logging.FileHandler(logfile),\n",
        "        logging.StreamHandler(sys.stdout)\n",
        "    ])\n",
        "\n",
        "logger = logging.getLogger()\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "if torch.cuda.device_count() > 1:\n",
        "    multi_gpu = True\n",
        "else:\n",
        "    multi_gpu = False"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'apex' already exists and is not an empty directory.\n",
            "/content/gdrive/My Drive/Colab_Notebooks/code/apex\n",
            "/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-nh1nrefx\n",
            "Created temporary directory: /tmp/pip-req-tracker-xtwah0z8\n",
            "Created requirements tracker '/tmp/pip-req-tracker-xtwah0z8'\n",
            "Created temporary directory: /tmp/pip-install-sgv7wdi1\n",
            "Processing /content/gdrive/My Drive/Colab_Notebooks/code/apex\n",
            "  Created temporary directory: /tmp/pip-req-build-tce34ahc\n",
            "  Added file:///content/gdrive/My%20Drive/Colab_Notebooks/code/apex to build tracker '/tmp/pip-req-tracker-xtwah0z8'\n",
            "    Running setup.py (path:/tmp/pip-req-build-tce34ahc/setup.py) egg_info for package from file:///content/gdrive/My%20Drive/Colab_Notebooks/code/apex\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.7.0+cu101\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-tce34ahc/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-tce34ahc/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-tce34ahc/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-tce34ahc/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-tce34ahc/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file '/tmp/pip-req-build-tce34ahc/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-tce34ahc/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-tce34ahc has version 0.1, which satisfies requirement apex==0.1 from file:///content/gdrive/My%20Drive/Colab_Notebooks/code/apex\n",
            "  Removed apex==0.1 from file:///content/gdrive/My%20Drive/Colab_Notebooks/code/apex from build tracker '/tmp/pip-req-tracker-xtwah0z8'\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-record-85kzz3fz\n",
            "    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-tce34ahc/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-tce34ahc/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-85kzz3fz/install-record.txt --single-version-externally-managed --compile\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.7.0+cu101\n",
            "\n",
            "\n",
            "    /tmp/pip-req-build-tce34ahc/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "    Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "    Cuda compilation tools, release 10.1, V10.1.243\n",
            "    from /usr/local/cuda/bin\n",
            "\n",
            "    running install\n",
            "    running build\n",
            "    running build_py\n",
            "    creating build\n",
            "    creating build/lib.linux-x86_64-3.6\n",
            "    creating build/lib.linux-x86_64-3.6/apex\n",
            "    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    creating build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    creating build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    creating build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    running build_ext\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:339: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "    building 'apex_C' extension\n",
            "    creating build/temp.linux-x86_64-3.6\n",
            "    creating build/temp.linux-x86_64-3.6/csrc\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/flatten_unflatten.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    In file included from csrc/flatten_unflatten.cpp:2:0:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h: In member function ‘at::DeprecatedTypeProperties& torch::utils::TensorGroup::type()’:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h:36:28: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         return tensors[0].type();\n",
            "                                ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/flatten_unflatten.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'amp_C' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/amp_C_frontend.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'syncbn' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/syncbn.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'fused_layer_norm_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:129:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:149:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:150:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:151:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:193:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:194:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:195:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:196:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:218:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:219:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:220:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:221:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:222:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:223:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'mlp_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.6/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:56:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:64:77: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n",
            "                                                                                 ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                        ^\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:67:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:113:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:119:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < inputs.size(); i++) {\n",
            "                       ~~^~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:120:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:123:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/cuda/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda/include -I/usr/include/python3.6m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/mlp.o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    running install_lib\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/sparse_masklib.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/asp.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adagrad.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/mlp.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/asp.py to asp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/models.py to models.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/db.py to db.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/data.py to data.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/output.py to output.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/base.py to base.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/mlp.py to mlp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/opt.py to opt.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/utils.py to utils.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/handle.py to handle.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/amp.py to amp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/compat.py to compat.cpython-36.pyc\n",
            "    running install_egg_info\n",
            "    running egg_info\n",
            "    creating apex.egg-info\n",
            "    writing apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to apex.egg-info/top_level.txt\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    Copying apex.egg-info to /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6.egg-info\n",
            "    running install_scripts\n",
            "    writing list of installed files to '/tmp/pip-record-85kzz3fz/install-record.txt'\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
            "  Removing source in /tmp/pip-req-build-tce34ahc\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-xtwah0z8'\n",
            "/content/gdrive/My Drive/Colab_Notebooks/code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EdR4a-QwQIF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "368a7c79-e08d-483d-898d-514bbd611569"
      },
      "source": [
        "pip install pytorch-pretrained-bert"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 22.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 28.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 23.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 20.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 23.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 19.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 18.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 19.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 20.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 20.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 20.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 20.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 20.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/f7/67b17c83da4e3f3c258aa42f98d5a86e931a932abe3f98dba01b7ad8e379/boto3-1.16.42-py2.py3-none-any.whl (130kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 44.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.19.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2020.12.5)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.20.0,>=1.19.42\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/f2/a14bb92df7fa19db3a43ff059dd290856da74351002157d60f184f98cb0d/botocore-1.19.42-py2.py3-none-any.whl (7.2MB)\n",
            "\u001b[K     |████████████████████████████████| 7.2MB 50.2MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.42->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.42->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "\u001b[31mERROR: botocore 1.19.42 has requirement urllib3<1.27,>=1.25.4; python_version != \"3.4\", but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.16.42 botocore-1.19.42 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UPCpIKpC7Ut",
        "outputId": "43680a10-1fd3-4661-a964-00be1de3957d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(torch.cuda.is_available())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxTo7cgvvy_C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be19ec93-d051-4589-f95f-c580f734ee59"
      },
      "source": [
        "from pytorch_pretrained_bert import BertTokenizer,BertModel\n",
        "import torch\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "a = \"i am a dog\"\n",
        "a_tokens = bert_tokenizer.tokenize(a)\n",
        "print(a_tokens)\n",
        "a_seq_ids = bert_tokenizer.convert_tokens_to_ids(a_tokens)\n",
        "print(a_seq_ids)\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(\"cuda\")\n",
        "batch_data = torch.Tensor(a_seq_ids).cuda().long().view((1,-1))\n",
        "out,_ = bert_model(batch_data)\n",
        "print(out[0].shape)\n",
        "print(out[0][0][0])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/23/2020 05:27:43 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "['i', 'am', 'a', 'dog']\n",
            "[1045, 2572, 1037, 3899]\n",
            "12/23/2020 05:27:44 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "12/23/2020 05:27:44 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpqh5_e3qu\n",
            "12/23/2020 05:27:47 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "torch.Size([1, 4, 768])\n",
            "tensor([ 1.0958e-01,  3.9581e-01, -1.8567e-01, -4.2336e-01,  1.8377e-01,\n",
            "        -1.8558e-01, -1.3111e-01, -4.2707e-01, -5.4226e-02, -6.7441e-01,\n",
            "        -1.6811e-01,  1.6283e-01,  3.6145e-01, -5.4008e-01, -6.1346e-01,\n",
            "         6.0326e-01,  3.0015e-01, -4.9211e-01,  9.0193e-02, -2.9138e-01,\n",
            "        -4.6905e-01, -1.4986e-01, -1.8168e-01,  1.7911e-01,  1.1844e-01,\n",
            "         6.2472e-02, -1.3374e-01,  1.1768e-01, -1.4904e-01, -4.9273e-02,\n",
            "        -4.0885e-01, -5.5264e-01,  2.2535e-01,  3.0552e-01, -1.3840e-01,\n",
            "         1.0465e-01, -1.6148e-01,  3.0482e-01, -2.7219e-01,  2.4715e-01,\n",
            "         4.4474e-01, -3.8584e-01, -2.1029e-01, -2.1222e-01, -3.2275e-02,\n",
            "         3.2474e-01, -1.8795e+00, -1.4722e-01,  2.7802e-01, -1.6842e-01,\n",
            "         2.9510e-01, -3.9545e-01, -2.8469e-01,  1.4557e-01,  2.5685e-01,\n",
            "         2.8473e-01, -3.0699e-01, -7.5099e-02,  1.0099e-01, -5.7150e-02,\n",
            "         1.4207e-01, -1.7746e-01, -8.7022e-01,  2.0627e-01, -2.9027e-01,\n",
            "         1.4402e-01, -2.0310e-01,  8.6504e-02,  2.6067e-02,  4.4306e-01,\n",
            "         2.4149e-01,  2.3797e-01, -2.2877e-02, -4.8521e-01,  1.4587e-01,\n",
            "         6.4696e-02,  6.4761e-02,  3.2247e-01, -1.0523e-01,  2.6463e-01,\n",
            "        -1.4856e-01,  2.2769e-01,  1.9493e-02, -1.2913e-01, -1.7073e-01,\n",
            "        -1.6694e-01,  4.4366e-03, -1.7618e-01, -2.9019e-03, -3.0856e-01,\n",
            "        -3.8548e-01,  3.1436e-02, -4.1474e-01,  2.2233e-01,  1.0028e-01,\n",
            "         7.1429e-02, -2.6711e-02, -1.2833e-01, -1.3906e-01, -1.8678e-01,\n",
            "         5.1791e-03,  2.1039e-01,  1.5651e-01, -7.2497e-01, -2.2174e-01,\n",
            "         6.9549e-01,  1.8756e-01,  1.6628e-01, -1.8062e-01, -1.0579e-01,\n",
            "         2.2376e-01, -1.6326e-01, -2.1031e-01,  6.8184e-02,  2.3134e-01,\n",
            "         1.5543e-01, -4.1632e-01,  2.7327e-02,  2.5750e-01, -1.2819e-01,\n",
            "        -1.8880e-01,  8.7392e-01, -8.7134e-02,  1.6168e-01, -2.4255e-01,\n",
            "        -2.3260e-01,  7.2086e-02, -6.0192e-01,  1.9610e-01,  1.9247e-01,\n",
            "        -1.4554e-01,  8.6008e-02,  5.9449e-01, -1.0859e-01, -4.6755e-01,\n",
            "        -2.2463e-01,  2.2718e-01,  7.0813e-03,  1.0311e-01,  1.3693e+00,\n",
            "         1.3353e-01, -8.9448e-02, -5.8468e-01,  1.0046e-01, -1.1030e-02,\n",
            "        -1.6079e-02,  4.9108e-02, -4.9471e-03,  3.7328e-01,  1.4733e-01,\n",
            "         1.2889e-01,  2.5007e-02, -1.4095e-01, -2.0632e-02, -2.7993e-01,\n",
            "        -2.3610e-01,  2.5874e-01,  8.0398e-01, -1.5061e-01,  2.0299e-01,\n",
            "        -8.3124e-02,  1.0825e-01, -8.6426e-02,  7.5193e-04,  2.3989e-01,\n",
            "         2.5836e-01,  4.7841e-01,  7.8087e-02, -1.6739e-01, -7.0784e-03,\n",
            "         7.6480e-02, -2.2387e-01,  3.4996e-01, -3.7206e-01,  2.1758e-02,\n",
            "         2.1016e-01,  1.9147e-01, -1.1657e-01,  1.8953e-01, -1.1593e-01,\n",
            "         1.6244e-01, -1.4406e-02, -4.0520e-01, -1.5017e-01,  2.5194e-01,\n",
            "        -2.3147e-03, -7.6210e-03, -1.0432e-01,  1.8149e-01, -6.3783e-02,\n",
            "         1.1538e-01, -5.6868e-02, -4.2823e-02,  3.4295e-01, -4.8545e-01,\n",
            "        -5.1621e-01,  2.7756e-01,  5.1273e-01, -6.8025e-03, -1.4109e-01,\n",
            "         3.6670e-02, -5.0432e-02, -4.6547e-02, -4.8438e-02,  1.8612e-01,\n",
            "        -8.8903e-02,  4.8368e-02,  2.5637e-01, -5.5249e-01,  1.1637e-01,\n",
            "         1.9745e-01, -2.0898e-01,  2.6937e-01, -5.1316e-02,  1.4303e-01,\n",
            "        -4.2006e-01,  1.5398e-01, -1.9457e-01,  2.3095e-01, -1.6776e-01,\n",
            "         3.1864e-01,  1.2887e-01, -5.9917e-01,  5.1008e-01,  8.2391e-02,\n",
            "         3.0657e-01,  6.9248e-02,  2.3775e-01, -1.5771e-01, -6.4167e-01,\n",
            "         4.7755e-01, -2.0043e-01,  1.1162e-01,  1.9169e-01,  2.9482e-01,\n",
            "         2.1753e-01,  1.3368e-01, -3.7343e-01, -2.3710e-01, -1.6300e-01,\n",
            "        -2.4718e-02,  1.0452e-02, -4.4930e-01,  2.6426e-01,  2.7336e-01,\n",
            "        -5.4467e-01,  1.9947e-01,  5.1857e-01,  3.9131e-01,  8.2537e-02,\n",
            "        -2.8520e-01, -1.0365e-01, -7.6373e-02,  4.6511e-01,  3.4038e-01,\n",
            "         1.7719e-01,  5.9061e-01,  3.5024e-01, -3.4132e-01,  1.6509e-01,\n",
            "        -4.0221e-02, -2.7421e-01,  7.7721e-01,  3.2800e-01, -3.4330e-01,\n",
            "        -8.6153e-02,  8.8285e-02, -1.0707e-01,  6.3707e-01,  3.5361e-01,\n",
            "        -4.2977e-01,  5.4062e-01, -3.9992e-02,  3.4913e-01, -2.8208e-01,\n",
            "        -4.5644e-01, -8.7301e-02,  1.9546e-01, -2.4422e-01, -3.7585e-01,\n",
            "        -1.1330e-01,  2.7538e-01,  4.3276e-01,  2.0466e-01,  2.5300e-01,\n",
            "        -3.0282e-01, -5.1473e-02, -2.0083e-01,  5.6709e-02,  1.5723e-01,\n",
            "        -2.5351e-01,  3.2740e-01, -1.9241e-01,  2.3112e-01, -1.1646e-01,\n",
            "         5.7189e-01,  9.6889e-02,  5.5575e-02,  2.1317e-01, -8.5738e-03,\n",
            "        -3.0032e-01,  5.8156e-02,  1.7051e-01, -2.5038e-01, -4.8526e-01,\n",
            "        -2.8683e-01, -3.0971e-01,  5.1153e-02, -9.8885e+00,  9.2412e-02,\n",
            "        -2.6286e-01,  5.1525e-02,  1.7554e-01,  2.7831e-01, -8.8233e-02,\n",
            "         4.6765e-02,  1.1260e-01,  3.8815e-01,  2.0313e-01,  3.4631e-01,\n",
            "         1.9976e-02,  1.0750e-01,  3.4727e-01, -1.4008e-01, -2.1210e-01,\n",
            "        -1.2704e-01, -2.8492e-01, -9.5711e-03,  4.2642e-01, -2.0138e-01,\n",
            "         1.9831e-01, -4.0246e-01, -2.8739e-01, -3.7663e-02, -5.1024e-01,\n",
            "         2.9440e-01, -1.1139e-01,  4.4392e-01, -2.4232e-01, -3.0696e-02,\n",
            "         9.4612e-02,  4.5880e-01,  5.1958e-01,  1.7925e-01,  8.3958e-02,\n",
            "         5.3326e-01, -4.6325e-03,  8.9622e-02, -2.5417e-03,  1.2622e-01,\n",
            "        -2.4136e-01,  3.5139e-01,  1.9154e-01, -3.2943e-01,  2.0514e-01,\n",
            "        -2.8315e-01, -2.8001e-01,  1.3874e-01, -2.4051e-01, -1.9749e-01,\n",
            "         3.4786e-01,  1.2419e-01, -1.6668e-01, -2.9891e-01, -3.1901e-01,\n",
            "         2.4017e-01, -2.9811e-01,  3.1450e-01, -3.9048e-02, -3.1055e-02,\n",
            "        -2.7270e-01, -3.8003e-01,  6.6261e-01, -7.6084e-02,  2.5822e-02,\n",
            "        -3.8534e-02, -2.9101e-01,  6.3840e-03, -4.8152e-02, -2.5156e-01,\n",
            "        -5.2966e-02, -1.0880e-01, -1.6068e-01,  3.8796e-03,  3.9520e-01,\n",
            "         2.8538e-01,  1.4836e-01,  5.1005e-02,  8.1831e-02,  1.5671e-01,\n",
            "        -2.0113e-01,  1.2601e-01,  1.2854e-02, -7.6175e-01, -5.7863e-01,\n",
            "        -4.4567e-01,  4.1706e-02, -4.1629e-01,  3.8636e-01,  4.4160e-01,\n",
            "         1.6577e-01,  2.0536e-01,  1.9058e-03, -4.7742e-01, -3.3444e-01,\n",
            "        -1.7264e-01,  3.9773e-02,  1.0968e-01,  2.9422e-01, -1.7256e-02,\n",
            "         1.4051e-02, -2.1231e-01,  1.9026e-01,  4.3591e-02,  2.7282e-01,\n",
            "         6.8066e-02, -2.8874e-01,  8.2023e-02,  3.1115e-01,  3.6968e-01,\n",
            "        -8.0774e-02, -1.0924e-01,  1.5893e-01, -5.5751e-02, -2.0861e-02,\n",
            "         1.2410e-01,  4.4199e-01, -6.2873e-01, -3.7197e-01, -7.4116e-02,\n",
            "         1.2019e-01, -2.8143e-01, -5.7933e-02,  2.0047e-01, -2.8958e-01,\n",
            "         9.6797e-02,  1.0045e-01,  8.4446e-02,  2.1221e-01,  3.0814e-01,\n",
            "        -1.8988e-01,  8.4283e-02,  2.2278e-01,  8.3495e-01,  3.2232e-02,\n",
            "        -2.4024e-01, -6.0686e-01,  1.6112e-01, -1.5025e-01, -5.4438e-02,\n",
            "         3.5094e-02, -6.2530e-03, -6.5409e-02,  4.3612e-01, -5.2534e-01,\n",
            "         1.7813e-01, -3.3028e-01,  1.7165e-01,  4.2750e-01,  3.2645e-01,\n",
            "        -2.3127e-01,  3.6166e-01, -5.4068e-02,  8.0452e-02,  2.0150e-02,\n",
            "        -2.4781e-01,  4.4065e-01, -1.9175e-01, -1.2347e-01, -1.6734e-01,\n",
            "        -7.3665e-01, -3.5916e-01,  4.1826e-01, -8.0608e-02, -4.8487e-01,\n",
            "         3.0515e-01,  1.8879e-01, -2.3604e-01, -4.5124e-01, -2.3019e-01,\n",
            "         4.3522e-01,  1.3242e-01, -6.0228e-01, -4.1689e-02, -4.6635e-01,\n",
            "        -5.0496e-02,  2.2157e-01,  2.7956e-01,  3.1454e-01,  3.9895e-02,\n",
            "         7.7137e-02,  4.1179e-01,  2.7650e-01, -6.3155e-02,  1.9532e-01,\n",
            "        -1.2520e-01, -1.7790e-01,  5.8012e-01,  4.7571e-02, -6.5974e-01,\n",
            "        -4.7743e-01,  1.4874e-01, -8.1102e-02, -4.0236e-01, -1.5526e-01,\n",
            "        -4.0264e-02, -2.3605e-01,  1.4059e-01, -2.5052e-02, -2.0372e-01,\n",
            "        -4.1908e-02,  2.5538e-02, -4.3202e-01, -2.1102e-01, -4.4943e-01,\n",
            "         3.8174e-01, -3.7685e-01, -3.6255e-01, -1.1878e-01,  3.7945e-01,\n",
            "        -2.1085e-01, -2.1744e-03,  4.1573e-01,  1.5723e-02,  5.1828e-02,\n",
            "         1.3497e-01, -1.5065e-01, -1.3287e-01, -2.8753e-01, -1.5999e-01,\n",
            "         1.2187e-01, -3.9536e-01, -2.2747e-01, -6.2390e-02, -8.4856e-02,\n",
            "        -5.4997e-01, -4.0252e-01,  9.9036e-02,  1.0053e-01, -9.0383e-01,\n",
            "         9.8502e-02,  2.3401e-01, -1.4681e-01,  1.5676e-01, -3.3298e-02,\n",
            "         3.8128e-01, -7.5555e-03, -2.0802e-01, -4.5027e-01,  2.2158e-01,\n",
            "        -1.9508e-01, -1.2215e-01,  1.9929e-01, -4.7000e-01, -3.0413e-01,\n",
            "        -9.5039e-02, -4.0637e-01, -2.2022e-01, -1.3205e-01,  3.1103e-01,\n",
            "        -8.5728e-01,  1.6102e-02,  1.4704e-03,  8.9413e-02,  6.1294e-02,\n",
            "         3.0769e-01,  2.5399e-02,  1.3162e-01,  2.4586e-01, -8.3480e-03,\n",
            "         2.4838e-01,  4.4649e-01, -2.3648e-01,  1.0957e-01,  3.8557e-01,\n",
            "         1.4698e-01,  2.2861e-01, -1.7368e-01,  3.5772e-01, -9.3909e-02,\n",
            "         6.4024e-02, -8.7066e-03,  5.3182e-02, -9.2972e-02, -4.2803e-01,\n",
            "         1.8725e-01, -1.1994e-01, -1.1439e-01, -2.3414e-01,  4.2477e-01,\n",
            "         1.7394e-01,  3.5732e-01,  2.0664e-01, -3.0476e-01, -1.7628e-01,\n",
            "        -2.4810e-01, -9.9070e-02,  9.8981e-02,  1.7951e-01,  3.5360e-01,\n",
            "        -1.9019e-01,  1.8319e-01,  6.6661e-02,  4.5493e-01,  5.0836e-02,\n",
            "        -4.2354e-01, -1.7642e-01,  3.0460e-01, -2.1502e-01,  5.9122e-02,\n",
            "        -2.4444e-01,  5.3425e-02, -1.9238e-01,  4.3538e-01, -1.6328e-02,\n",
            "        -2.1223e-01,  2.0990e-01, -1.1569e-02,  2.0108e-01, -5.0092e-02,\n",
            "         3.9690e-01,  1.1383e-01, -2.2841e-01, -2.5906e-02,  3.4032e-02,\n",
            "         1.5066e-01,  3.1504e-01, -1.7136e-01, -2.4651e-02, -1.7344e-01,\n",
            "         1.0688e-01, -6.6259e-01, -3.6300e-01,  2.7047e-01,  1.3320e-01,\n",
            "         5.0318e-02,  2.7541e-01,  4.9102e-01, -1.2390e-01, -2.3882e-01,\n",
            "        -4.4520e-01, -2.4944e-02,  5.1079e-02,  2.6391e-02, -2.1842e-01,\n",
            "         1.2620e-01, -1.5588e-01,  5.6157e-02, -4.0520e-01, -1.1395e-01,\n",
            "         4.9729e-01,  9.9568e-02,  3.3872e-01, -6.3505e-02,  2.3689e-02,\n",
            "        -4.0508e-02,  1.5441e-01, -1.6671e-01, -1.1312e-02, -4.4992e-02,\n",
            "         1.7681e-02,  2.1162e-01, -1.1003e-01,  6.6100e-02,  9.8899e-02,\n",
            "         2.8389e-01, -5.3644e-01, -1.5957e-01, -2.4247e-01,  2.4969e-01,\n",
            "        -3.5119e-01, -3.3684e-01, -7.6941e-02,  4.6991e-02, -2.7552e-01,\n",
            "        -2.1473e-01, -3.6750e-01,  1.3948e-01,  1.9795e-01,  5.8185e-01,\n",
            "         7.5371e-02,  6.3070e-01, -7.0589e-01,  1.1211e-01, -4.9678e-02,\n",
            "        -1.4062e-02, -5.7266e-02, -3.4960e-02, -1.5285e-01,  1.3307e-01,\n",
            "        -9.3159e-02,  1.8862e-01, -4.7597e-01, -1.5642e-01,  7.4020e-03,\n",
            "         6.7772e-02, -1.9558e-01, -1.8109e-01, -4.9247e-01, -6.2188e-01,\n",
            "        -1.8623e-01,  2.5905e-02,  5.2444e-02, -6.1293e-02,  4.8909e-01,\n",
            "         7.3964e-02,  7.1087e-01, -1.1553e-01,  1.0139e-01,  1.3109e-01,\n",
            "         3.6475e-01, -3.6684e-01,  2.9059e-01,  2.9076e-01, -2.7127e-01,\n",
            "         1.9484e-02, -9.5180e-02, -2.4850e-01,  5.2973e-02,  2.6679e-01,\n",
            "        -2.6135e-01,  2.6845e-01, -2.0397e-01, -3.3965e-01, -1.9095e-01,\n",
            "        -3.2373e-02, -3.5023e-01, -4.1801e-01,  1.6451e-02,  1.8130e-01,\n",
            "        -1.4091e-01,  6.8014e-01, -6.9271e-03, -6.8973e-01,  1.2148e-01,\n",
            "        -4.0088e-01, -6.5048e-02, -3.0489e-01,  6.1890e-02, -1.6376e-01,\n",
            "        -1.1055e-01,  1.9633e-01,  1.4595e-01,  2.1144e-01, -4.0406e-03,\n",
            "         1.6351e-01,  4.6402e-01, -9.1207e-02,  2.7025e-01,  1.2631e-01,\n",
            "         8.5206e-02, -2.4323e-01, -1.7688e-01, -1.4498e-01,  2.5580e-01,\n",
            "         5.1842e-01, -2.4841e-01, -1.0730e-01,  2.5647e-01, -1.8675e-01,\n",
            "         3.4521e-01, -3.5921e-01,  5.8237e-01, -4.8096e-01, -3.1416e-01,\n",
            "         2.3217e-02,  2.9314e-01,  1.0521e-01], device='cuda:0',\n",
            "       grad_fn=<SelectBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3xf12_U9-NO",
        "outputId": "ddb0eb15-d5c3-4d25-b01b-d526877ae548",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from pytorch_pretrained_bert import BertTokenizer,BertModel\n",
        "import torch\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\").to(\"cuda\")\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "import numpy as np\n",
        "import csv\n",
        "label_list = [\"GoodsServices\",\n",
        "        \"SearchAndRescue\",\n",
        "        \"InformationWanted\",\n",
        "        \"Volunteer\",\n",
        "        \"FundRaising\",\n",
        "        \"Donations\",\n",
        "        \"MovePeople\",\n",
        "        \"FirstPartyObservation\",\n",
        "        \"ThirdPartyObservation\",\n",
        "        \"Weather\",\n",
        "        \"EmergingThreats\",\n",
        "        \"NewSubEvent\",\n",
        "        \"MultimediaShare\",\n",
        "        \"ServiceAvailable\",\n",
        "        \"Factoid\",\n",
        "        \"Official\",\n",
        "        \"CleanUp\",\n",
        "        \"Hashtags\",\n",
        "        \"ContextualInformation\",\n",
        "        \"News\",\n",
        "        \"Advice\",\n",
        "        \"Sentiment\",\n",
        "        \"Discussion\",\n",
        "        \"Irrelevant\",\n",
        "        \"OriginalEvent\"]\n",
        "important_list = [\"Low\",\n",
        "        \"Medium\",\n",
        "        \"High\",\n",
        "        \"Critical\"\n",
        "       ]\n",
        "tweets = []\n",
        "path = '/content/gdrive/My Drive/Colab_Notebooks/code/bert-version/src/attention/data/ll.csv'\n",
        "with open(path, 'r', encoding='utf-8') as f:\n",
        "    reader = csv.reader(f)\n",
        "    for line in reader:\n",
        "        tweet_full = line\n",
        "        tweets.append({\n",
        "            'id': tweet_full[0],\n",
        "            'label':tweet_full[1],\n",
        "            'important':tweet_full[2],\n",
        "            'text': tweet_full[3].lower(),\n",
        "            # 'name': tweet_full['user']['name'].split()[0]\n",
        "            })\n",
        "tweet_list = []\n",
        "text_list = []\n",
        "label_list = []\n",
        "for i in range(len(tweets)):\n",
        "        #text_list.append(tweets[i]['text'])\n",
        "        a_tokens = bert_tokenizer.tokenize(tweets[i]['text'])\n",
        "        a_seq_ids = bert_tokenizer.convert_tokens_to_ids(a_tokens)\n",
        "        batch_data = torch.Tensor(a_seq_ids).cuda().long().view((1,-1))\n",
        "        out,_ = bert_model(batch_data)\n",
        "        print(out[0].shape)\n",
        "        tweets_vector = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
        "        tweets_label = eval(tweets[i]['label'])\n",
        "        p = 0\n",
        "        for j in label_list:\n",
        "            for k in tweets_label:\n",
        "                if k == j:\n",
        "                    tweets_vector[p] = 1\n",
        "            p+=1\n",
        "        label_list.append(tweets_vector)\n",
        "        text_list.append(out[0])\n",
        "numpy_text_list = np.array(text_list)\n",
        "numpy_label_text_list = np.array(label_list)\n",
        "np.save('./data/bert_label.npy',numpy_label_list)\n",
        "np.save('./data/bert_text.npy',numpy_text_list)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12/23/2020 05:52:56 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
            "12/23/2020 05:52:56 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp4aa6jcxb\n",
            "12/23/2020 05:53:00 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "12/23/2020 05:53:03 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 15, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 9, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 5, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 7, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 15, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 55, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 5, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 7, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 15, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 7, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 54, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 11, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 7, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 11, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 58, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 71, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 56, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 5, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 57, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 9, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 9, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 11, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 4, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([1, 6, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 9, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 9, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 56, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 7, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 59, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 63, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 54, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 15, 768])\n",
            "torch.Size([1, 55, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 58, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 15, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 54, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 57, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 55, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 11, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 56, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 15, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 59, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 55, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 11, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 11, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 55, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 56, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 9, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 56, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 54, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 54, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 57, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 15, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 9, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 65, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 57, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 11, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 55, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 15, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 55, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 8, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 15, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 7, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 6, 768])\n",
            "torch.Size([1, 7, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 4, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 58, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 59, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 65, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 3, 768])\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 8, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 5, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 58, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 15, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 11, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 55, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 8, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 4, 768])\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 15, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 7, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 7, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 9, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 8, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 7, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 3, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 7, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 56, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 3, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 6, 768])\n",
            "torch.Size([1, 15, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 7, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 5, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 4, 768])\n",
            "torch.Size([1, 55, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 5, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 4, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 8, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 11, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 56, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 5, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 7, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 11, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 5, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 3, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 6, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 6, 768])\n",
            "torch.Size([1, 3, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 4, 768])\n",
            "torch.Size([1, 8, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 3, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 54, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 7, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 54, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 4, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 15, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 60, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 8, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 57, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 4, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 9, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 55, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 7, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 3, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 8, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 9, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 8, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 2, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 8, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 3, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 7, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 8, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 11, 768])\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 6, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 11, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 7, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 8, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 4, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 5, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 9, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 4, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 6, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([1, 6, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 8, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 15, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 7, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 4, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 15, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 61, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 4, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 6, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 9, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 7, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 11, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 8, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 11, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 7, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 3, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 57, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 15, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 7, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 3, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 8, 768])\n",
            "torch.Size([1, 4, 768])\n",
            "torch.Size([1, 4, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 15, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 11, 768])\n",
            "torch.Size([1, 8, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 3, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 8, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 58, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 4, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 5, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 9, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 11, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 15, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 3, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 54, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 60, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 4, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 55, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 4, 768])\n",
            "torch.Size([1, 4, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 8, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 9, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 3, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 9, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 5, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 6, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 6, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 8, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 61, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 11, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 4, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 7, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 4, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 5, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 9, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([1, 11, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 6, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 5, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 15, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 11, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 64, 768])\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 8, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 9, 768])\n",
            "torch.Size([1, 54, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 7, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 56, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 55, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 55, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 6, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 4, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 58, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 3, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 54, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 9, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 9, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 54, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 56, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 54, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 55, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 11, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 19, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 16, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 15, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 55, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 58, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 54, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 9, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 57, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 54, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 55, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 51, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 13, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 60, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 6, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 56, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 18, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 15, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 31, 768])\n",
            "torch.Size([1, 59, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 57, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 21, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 54, 768])\n",
            "torch.Size([1, 17, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 14, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 47, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 50, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 26, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 15, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 41, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 23, 768])\n",
            "torch.Size([1, 45, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 20, 768])\n",
            "torch.Size([1, 12, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 53, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 49, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 46, 768])\n",
            "torch.Size([1, 37, 768])\n",
            "torch.Size([1, 34, 768])\n",
            "torch.Size([1, 29, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 27, 768])\n",
            "torch.Size([1, 32, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 48, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 28, 768])\n",
            "torch.Size([1, 40, 768])\n",
            "torch.Size([1, 38, 768])\n",
            "torch.Size([1, 43, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 36, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 33, 768])\n",
            "torch.Size([1, 52, 768])\n",
            "torch.Size([1, 35, 768])\n",
            "torch.Size([1, 39, 768])\n",
            "torch.Size([1, 10, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 30, 768])\n",
            "torch.Size([1, 25, 768])\n",
            "torch.Size([1, 22, 768])\n",
            "torch.Size([1, 8, 768])\n",
            "torch.Size([1, 42, 768])\n",
            "torch.Size([1, 44, 768])\n",
            "torch.Size([1, 45, 768])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-be4fce5788d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0ma_seq_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_seq_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mtweets_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    731\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[1;32m    732\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                                       output_all_encoded_layers=output_all_encoded_layers)\n\u001b[0m\u001b[1;32m    734\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mgelu\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mAlso\u001b[0m \u001b[0msee\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0marxiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1606.08415\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \"\"\"\n\u001b[0;32m--> 124\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 14.73 GiB total capacity; 13.57 GiB already allocated; 3.88 MiB free; 13.79 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    }
  ]
}